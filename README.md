#  SmartQuery: Offline Document-Based Question Answering System (RAG)

**SmartQuery** is a **Retrieval-Augmented Generation (RAG)** system that answers user questions based on the content of uploaded research papers â€” **completely offline**.

Unlike API-based systems, SmartQuery uses **local embeddings**, **FAISS-based vector search**, and **offline LLMs (via Ollama)** to ensure **data privacy**, **speed**, and **reliability**.  
This makes it ideal for **research**, **academia**, and **enterprise environments** where privacy and transparency are crucial.

---

### ğŸ” What It Does
SmartQuery enables users to:
- Upload **PDF research papers** or documents  
- Automatically **extract, embed, and store** their content  
- Ask **natural language questions**  
- Get **short, citation-aware answers** with **page references**

It integrates **semantic search (retrieval)** and **local language modeling (generation)** to simulate a fully autonomous, explainable AI assistant.


## ğŸ§© System Architecture

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   PDF Documents    â”‚
            â”‚ (Research Papers)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Text Extraction &  â”‚
         â”‚ Chunking (PyPDF2)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Sentence Embedding â”‚
         â”‚ (SentenceTransformers) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ FAISS Vector Index â”‚
         â”‚  (Semantic Search) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LLM (Ollama Phi)  â”‚
         â”‚   Offline Inferenceâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



---

## âš™ï¸ Tech Stack

| Component | Tool / Library | Purpose |
|------------|----------------|----------|
| **Text Extraction** | PyPDF2 | Extract text from PDFs |
| **Text Chunking** | Custom splitter | Maintain context in chunks |
| **Embeddings** | SentenceTransformers (`all-MiniLM-L6-v2`) | Convert text to numerical vectors |
| **Vector Search** | FAISS | Retrieve semantically similar chunks |
| **Language Model** | Ollama (`phi`, `llama3`, etc.) | Offline text generation |
| **Language** | Python | Core implementation |

---

## ğŸ’¡ Key Features

- âš¡ **100% Offline** â€” No internet or APIs required  
- ğŸ” **Semantic Search** â€” Understands meaning, not just keywords  
- ğŸ“„ **Page-Level Citations** â€” Trace every answer to source pages  
- ğŸ§± **Modular & Extensible** â€” Easily switch between models (phi, llama3)  
- ğŸ” **Privacy-Focused** â€” No cloud calls, runs entirely locally  
- ğŸ§  **Concise, Context-Aware Answers** â€” Ideal for academic Q&A  

---

## ğŸ› ï¸ Installation & Setup

### ğŸ–¥ï¸ Prerequisites
- Python **3.10+**
- Ollama installed â†’ [https://ollama.ai/download](https://ollama.ai/download)

---

### 1ï¸âƒ£ Clone this repository
```bash
git clone https://github.com/DishaJillella/SmartQuery.git
cd SmartQuery
````

---

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv venv
venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

If you donâ€™t have a `requirements.txt`, create one:

```text
PyPDF2
faiss-cpu
sentence-transformers
torch
tqdm
numpy
requests
```

---

### 4ï¸âƒ£ Pull a local model using Ollama

```bash
ollama pull phi
```

or (optional)

```bash
ollama pull llama3
```

---

### 5ï¸âƒ£ Add your research papers

Place all your PDFs in the `/papers` folder:

```
papers/
 â”œâ”€â”€ paper1.pdf
 â”œâ”€â”€ paper2.pdf
 â””â”€â”€ paper3.pdf
```

---

### 6ï¸âƒ£ Build the FAISS index

```bash
python build_index.py
```

This will:

* Extract text from PDFs
* Split it into small overlapping chunks
* Embed and store them in `vector.index` + `chunks.json`

---

### 7ï¸âƒ£ Run the Q&A system

```bash
python query_rag.py
```



##  How It Works

1. **Document Loading** â€“ Extracts text from PDFs in `/papers/`
2. **Chunking** â€“ Splits text into 800-character segments with overlaps
3. **Embeddings** â€“ Converts chunks into 384-dimensional semantic vectors
4. **FAISS Indexing** â€“ Builds a searchable database of all document embeddings
5. **Query Handling** â€“ User query is embedded â†’ matched â†’ context passed to LLM
6. **Answer Generation** â€“ The LLM (Phi) generates short, cited answers

---



## ğŸ¯ Use Cases

* Research paper summarization
* Legal or medical document Q&A
* Academic literature review assistant
* Private enterprise document analysis
* Offline AI assistant for restricted environments

---



## ğŸ”’ Privacy & Security

All processing â€” including embedding, retrieval, and generation â€” happens **locally**.
No data leaves your system.
This ensures maximum privacy and makes SmartQuery suitable for sensitive domains.

---

## ğŸš§ Future Enhancements

* âœ… Add a **Streamlit UI** for easy interaction
* âœ… Integrate **incremental document updates**
* âœ… Add support for **tables and images** in PDFs
* âœ… Extend to **Agentic RAG** (multi-step reasoning using LangGraph)

---

## ğŸ§¾ Credits

**Developed by:** Disha Jillella

**Institution:** CBIT, Hyderabad, Telangana

**Mentor:** [Dr. Y Ramadevi]

**Year:** 2025

**Technologies:** Python, FAISS, SentenceTransformers, Ollama
